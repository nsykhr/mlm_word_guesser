{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "punctuation += '«»—…“”*№–'\n",
    "punctuation = set(punctuation)\n",
    "stopwords = set(stopwords.words('russian'))\n",
    "\n",
    "\n",
    "class Guess:\n",
    "    \"\"\"\n",
    "    Этот класс используется для структурированного хранения предсказаний модели для маскированных токенов.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 words: List[str],\n",
    "                 probs: Union[List[float], np.ndarray]) -> None:\n",
    "        self.words = words\n",
    "        self.probs = np.array(probs) if isinstance(probs, list) else probs\n",
    "\n",
    "    @classmethod\n",
    "    def from_model_predictions(cls,\n",
    "                               probs: np.ndarray,\n",
    "                               tokenizer: BertTokenizer,\n",
    "                               max_len: int = 1000):\n",
    "        probs_with_indices = list(enumerate(probs))\n",
    "        best_probs_with_indices = sorted(probs_with_indices, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        best_words = {}\n",
    "        for i, prob in best_probs_with_indices:\n",
    "            word = tokenizer.convert_ids_to_tokens(i).lower()\n",
    "            if word in punctuation | stopwords or word in best_words:\n",
    "                continue\n",
    "\n",
    "            best_words[word] = prob\n",
    "\n",
    "            if len(best_words) == max_len:\n",
    "                break\n",
    "\n",
    "        best_probs = list(best_words.values())\n",
    "        best_words = list(best_words.keys())\n",
    "\n",
    "        return cls(best_words, best_probs)\n",
    "\n",
    "    def get_best_guess(self) -> Union[Tuple[str, float], None]:\n",
    "        return self.words[0], self.probs[0] if bool(self) else None\n",
    "\n",
    "    def get_n_best_guesses(self, n: int = 5) -> Union[List[str], None]:\n",
    "        return self.words[:n] if bool(self) else None\n",
    "\n",
    "    def __bool__(self) -> bool:\n",
    "        return bool(self.words) and self.probs.shape[0] == len(self.words)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return '\\n'.join([f'{word}: {prob:.2f}' for word, prob in zip(self.words, self.probs)])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.probs.shape[0]\n",
    "\n",
    "    def __add__(self, x):\n",
    "        if not self and not x:\n",
    "            return Guess([], [])\n",
    "        if not self:\n",
    "            return x\n",
    "        if not x:\n",
    "            return self\n",
    "\n",
    "        combined_words = set(self.words) & set(x.words)\n",
    "        if not combined_words:\n",
    "            return Guess([], [])\n",
    "\n",
    "        new_words = {}\n",
    "\n",
    "        i, j = 0, 0\n",
    "        while i < len(self) or j < len(x):\n",
    "            if j == len(x) or (i < len(self) and self.probs[i] > x.probs[j]):\n",
    "                if self.words[i] not in combined_words:\n",
    "                    i += 1\n",
    "                    continue\n",
    "                new_words[self.words[i]] = (new_words.get(self.words[i], self.probs[i]) + self.probs[i]) / 2\n",
    "                i += 1\n",
    "            else:\n",
    "                if x.words[j] not in combined_words:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                new_words[x.words[j]] = (new_words.get(x.words[j], x.probs[j]) + x.probs[j]) / 2\n",
    "                j += 1\n",
    "\n",
    "        new_len = min(len(self), len(x))\n",
    "        sorted_items = sorted(new_words.items(), key=lambda x: x[1], reverse=True)[:new_len]\n",
    "        new_words = [x[0] for x in sorted_items]\n",
    "        new_probs = np.array([x[1] for x in sorted_items])\n",
    "\n",
    "        return Guess(new_words, new_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = Guess(['мама', 'мыла', 'раму'], np.array([0.5, 0.3, 0.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мама: 0.50\n",
      "мыла: 0.30\n",
      "раму: 0.20\n"
     ]
    }
   ],
   "source": [
    "print(g1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('мама', 0.5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1.get_best_guess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = Guess(['мама', 'мыла', 'раму'], [0.7, 0.15, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мама: 0.60\n",
      "мыла: 0.22\n",
      "раму: 0.17\n"
     ]
    }
   ],
   "source": [
    "print(g1 + g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g3 = Guess([], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(g3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(g3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class MLMPredictor(ABC):\n",
    "    \"\"\"\n",
    "    Это абстрактный класс для предиктора MLM-модели.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 bert_path: str = 'rubert_cased_L-12_H-768_A-12_pt',\n",
    "                 device: str = 'cpu') -> None:\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_path, do_lower_case=False)\n",
    "        self.mask_token = self.tokenizer.mask_token\n",
    "        \n",
    "        self.model = BertForMaskedLM.from_pretrained(bert_path)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def tokenize(*args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(*args, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "from overrides import overrides\n",
    "\n",
    "\n",
    "class BERTMLMPredictor(MLMPredictor):\n",
    "    \"\"\"\n",
    "    Этот класс позволяет получить предсказания MLM-модели для маскированного токена.\n",
    "    \"\"\"\n",
    "    \n",
    "    @overrides\n",
    "    def tokenize(self, text: str) -> Tuple[List[str], int]:\n",
    "        tokens = self.tokenizer.tokenize(text.strip(), add_special_tokens = True)\n",
    "        \n",
    "        mask_indices = [i for i, token in enumerate(tokens) if token == self.mask_token]\n",
    "        if len(mask_indices) == 0:\n",
    "            raise IOError(f'The mask token is not found in model input: {text}.')\n",
    "        if len(mask_indices) > 1:\n",
    "            raise IOError(f'The mask token occurs more than once in model input: {text}.')\n",
    "        mask_idx = mask_indices[0]\n",
    "    \n",
    "        return tokens, mask_idx\n",
    "        \n",
    "    @overrides\n",
    "    def predict(self,\n",
    "                text: Union[str, List[str]]) -> Guess:\n",
    "        tokens, mask_idx = self.tokenize(text) if isinstance(text, str) \\\n",
    "            else self.tokenize(' '.join(text))\n",
    "        \n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(torch.tensor([token_ids]).to(self.device))[0][0]\n",
    "        \n",
    "        masked_probs = softmax(model_output[mask_idx].cpu().numpy())\n",
    "        guess = Guess.from_model_predictions(masked_probs, self.tokenizer)\n",
    "        \n",
    "        return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameHandler:\n",
    "    \"\"\"\n",
    "    Этот класс отвечает за взаимодействие с пользователем и координирует работу остальных классов.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 bert_path: str = 'rubert_cased_L-12_H-768_A-12_pt',\n",
    "                 max_rounds: int = 10,\n",
    "                 prob_threshold: float = 0.5,\n",
    "                 device: str = 'cpu') -> None:\n",
    "        self.predictor = BERTMLMPredictor(bert_path, device)\n",
    "        self.guess = Guess([], [])\n",
    "        self.max_rounds = max_rounds\n",
    "        self.prob_threshold = prob_threshold\n",
    "    \n",
    "    def __update_guess(self, text: str) -> None:\n",
    "        new_guess = self.predictor.predict(text)\n",
    "        self.guess += new_guess\n",
    "    \n",
    "    def __play_round(self) -> None:\n",
    "        text = input()\n",
    "        self.__update_guess(text)\n",
    "    \n",
    "    def play_game(self, n: int = 5) -> None:\n",
    "        for _ in range(self.max_rounds):\n",
    "            self.__play_round()\n",
    "            \n",
    "            if not self.guess:\n",
    "                print(f'Вы меня запутали, я сдаюсь.')\n",
    "                return\n",
    "            \n",
    "            best_word, best_prob = self.guess.get_best_guess()\n",
    "            if best_prob > self.prob_threshold or len(self.guess) == 1:\n",
    "                print(f'Я думаю, вы загадали слово \"{best_word}\".')\n",
    "                return\n",
    "        \n",
    "        best_guesses = self.guess.get_n_best_guesses(n)\n",
    "        if len(best_guesses) == 1:\n",
    "            print(f'Я думаю, вы загадали слово \"{best_guesses[0]}\".')\n",
    "            return\n",
    "        \n",
    "        best_guesses = list(map(lambda x: '\"' + x + '\"', best_guesses))\n",
    "        \n",
    "        print(f'Вот мои лучшие догадки: {\", \".join(best_guesses)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_handler = GameHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На Гавайских островах активен [MASK] Килауэа.\n",
      "Геологические образования на поверхности Земли, где магма выходит на поверхность, называют словом [MASK].\n",
      "Бурлящий [MASK] начал медленно извергать раскалённую лаву.\n",
      "На северо-западе Мордора, посреди плато Горгорот, находится [MASK] Ородруин, где Саурон выковал Кольцо Всевластья.\n",
      "Древний [MASK] Ородруин (в переводе с синдарина это означает \"гора алого пламени\") поглотил Голлума вместе с Кольцом Всевластья.\n",
      "На восточном побережье Сицилии находится действующий [MASK] Этна.\n",
      "[MASK], который ни разу не извергался в последние 10 тысяч лет, считается потерявшим свою активность, потухшим.\n",
      "В тот страшный день произошло извержение. [MASK] выплеснул на поверхность тонны пылающей лавы.\n",
      "Самый большой [MASK] в мире - это Мауна-Лоа.\n",
      "На границе Чили и Аргентины находится самый высокий действующий [MASK] на Земле.\n",
      "Я думаю, вы загадали слово \"вулкан\".\n"
     ]
    }
   ],
   "source": [
    "game_handler.play_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
